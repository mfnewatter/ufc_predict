{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape UFCStats.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 724 event URLs.\n"
     ]
    }
   ],
   "source": [
    "def get_event_links():\n",
    "    url = 'http://ufcstats.com/statistics/events/completed?page=all'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    event_links = []\n",
    "    for a in soup.select('a.b-link.b-link_style_black'):\n",
    "        href = a.get('href')\n",
    "        if href and 'event-details' in href:\n",
    "            event_links.append(href)\n",
    "    \n",
    "    return list(set(event_links))  # Remove duplicates\n",
    "\n",
    "# Test it\n",
    "event_urls = get_event_links()\n",
    "print(f\"Collected {len(event_urls)} event URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 725 events.\n",
      "Processing event 1/1: http://ufcstats.com/event-details/bd4389b71fdc0ce2\n",
      "Saved extended UFC fight data to 'ufc_fight_history_extended.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_event_links():\n",
    "    \"\"\"\n",
    "    Scrape the page with all completed events and return a list of event URLs.\n",
    "    \"\"\"\n",
    "    url = 'http://ufcstats.com/statistics/events/completed?page=all'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    event_links = []\n",
    "    # The event links are found in anchor tags with class 'b-link b-link_style_black'\n",
    "    for a in soup.select('a.b-link.b-link_style_black'):\n",
    "        href = a.get('href')\n",
    "        if href and 'event-details' in href:\n",
    "            event_links.append(href)\n",
    "    return list(set(event_links))  # Remove duplicates if any\n",
    "\n",
    "def get_event_details(soup):\n",
    "    \"\"\"\n",
    "    Given the BeautifulSoup object of an event page, extract event-level details.\n",
    "    \"\"\"\n",
    "    details = {}\n",
    "    # Assume event date and location are in <li> tags with class 'b-list__box-list-item'\n",
    "    items = soup.find_all('li', class_='b-list__box-list-item')\n",
    "    for item in items:\n",
    "        text = item.get_text(strip=True)\n",
    "        if 'Date:' in text:\n",
    "            details['Event Date'] = text.replace('Date:', '').strip()\n",
    "        elif 'Location:' in text:\n",
    "            details['Location'] = text.replace('Location:', '').strip()\n",
    "    return details\n",
    "\n",
    "def parse_table_dual_rows(table):\n",
    "    \"\"\"\n",
    "    Parses tables where each <td> has two <p> tags representing two fighters.\n",
    "    Returns a dict of stat_name: (fighter1_val, fighter2_val)\n",
    "    \"\"\"\n",
    "    headers = [th.get_text(strip=True) for th in table.find(\"thead\").find_all(\"th\")]\n",
    "    rows = table.find(\"tbody\").find_all(\"tr\")[0].find_all(\"td\")\n",
    "\n",
    "    data = {\"fighter_1\": {}, \"fighter_2\": {}}\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        values = rows[i].find_all(\"p\")\n",
    "        if len(values) == 2:\n",
    "            data[\"fighter_1\"][header] = values[0].get_text(strip=True)\n",
    "            data[\"fighter_2\"][header] = values[1].get_text(strip=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_fight_stats(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract fighter names\n",
    "    fighter_tags = soup.select(\".b-fight-details__person-name a\")\n",
    "    if len(fighter_tags) < 2:\n",
    "        return None\n",
    "    fighter_1 = fighter_tags[0].get_text(strip=True)\n",
    "    fighter_2 = fighter_tags[1].get_text(strip=True)\n",
    "\n",
    "    fight_data = {\n",
    "        \"fighter_1_name\": fighter_1,\n",
    "        \"fighter_2_name\": fighter_2,\n",
    "    }\n",
    "\n",
    "    # Extract metadata: method, round, time, referee, etc.\n",
    "    metadata_section = soup.select_one(\".b-fight-details__content\")\n",
    "    if metadata_section:\n",
    "        for item in metadata_section.select(\".b-fight-details__text-item\"):\n",
    "            label = item.select_one(\".b-fight-details__label\")\n",
    "            if label:\n",
    "                key = label.get_text(strip=True).replace(\":\", \"\").lower()\n",
    "                value = item.get_text(strip=True).replace(label.get_text(strip=True), \"\").strip()\n",
    "                fight_data[key] = value\n",
    "\n",
    "    # Extract fight stats summary table\n",
    "    tables = soup.find_all(\"table\", class_=\"b-fight-details__table\")\n",
    "    if len(tables) > 0:\n",
    "        fight_data[\"overall_stats\"] = parse_table_dual_rows(tables[0])\n",
    "\n",
    "    # Extract per-round summary stats\n",
    "    if len(tables) > 1:\n",
    "        fight_data[\"per_round_stats\"] = {}\n",
    "        round_tables = soup.select(\"table.b-fight-details__table.js-fight-table\")\n",
    "        for i, round_table in enumerate(round_tables):\n",
    "            fight_data[\"per_round_stats\"][f\"round_{i+1}\"] = parse_table_dual_rows(round_table)\n",
    "            \n",
    "\n",
    "    # Extract significant strikes breakdown (overall)\n",
    "    if len(tables) > 2:\n",
    "        fight_data[\"sig_strike_breakdown\"] = parse_table_dual_rows(tables[2])\n",
    "\n",
    "    return fight_data\n",
    "\n",
    "def get_fights_from_event(event_url):\n",
    "    \"\"\"\n",
    "    Given an event URL, scrape the event page to extract:\n",
    "      - Basic fight details (fighter names, winner, weight class, method, round, time)\n",
    "      - Event details (date and location)\n",
    "      - Detailed fighter statistics from the fight detail page\n",
    "    \"\"\"\n",
    "    fights = []\n",
    "    try:\n",
    "        response = requests.get(event_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get event-level details (date and location)\n",
    "        event_details = get_event_details(soup)\n",
    "        \n",
    "        # Locate the fight table on the event page.\n",
    "        table = soup.find('table', class_='b-fight-details__table')\n",
    "        if not table:\n",
    "            return fights\n",
    "\n",
    "        rows = table.find_all('tr')[1:]  # Skip header row\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            # Verify there are enough columns before indexing\n",
    "            if len(cols) < 10:\n",
    "                continue\n",
    "\n",
    "            # Basic fight details (you may need to adjust the indices based on the site’s structure)\n",
    "            fight = {\n",
    "                'Fighter A': cols[1].get_text(strip=True),\n",
    "                'Fighter B': cols[2].get_text(strip=True),\n",
    "                'Winner': cols[0].get_text(strip=True),\n",
    "                'Weight Class': cols[6].get_text(strip=True),\n",
    "                'Method': cols[7].get_text(strip=True),\n",
    "                'Round': cols[8].get_text(strip=True),\n",
    "                'Time': cols[9].get_text(strip=True),\n",
    "                'Event Date': event_details.get('Event Date', None),\n",
    "                'Location': event_details.get('Location', None)\n",
    "            }\n",
    "            \n",
    "            # Extract the fight detail URL – typically found in an <a> tag in the row.\n",
    "            fight_link_tag = row.find('a')\n",
    "            #print(f'fight_link_tag:{fight_link_tag}')\n",
    "            fight_detail_url = fight_link_tag['href'] if fight_link_tag else None\n",
    "            #print(f'fight_detail_url:{fight_detail_url}')\n",
    "            if fight_detail_url:\n",
    "                fight_stats = get_fight_stats(fight_detail_url)\n",
    "                fight.update(fight_stats)\n",
    "                time.sleep(1)  # Delay between fight detail requests\n",
    "            \n",
    "            fights.append(fight)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event {event_url}: {e}\")\n",
    "    return fights\n",
    "\n",
    "def scrape_ufc_data(limit_events=None):\n",
    "    \"\"\"\n",
    "    Scrape data for all events (or limit to a set number) and compile fight details.\n",
    "    \"\"\"\n",
    "    all_fights = []\n",
    "    event_urls = get_event_links()\n",
    "    print(f\"Found {len(event_urls)} events.\")\n",
    "    if limit_events:\n",
    "        event_urls = event_urls[:limit_events]\n",
    "    \n",
    "    for i, event_url in enumerate(event_urls, start=1):\n",
    "        print(f\"Processing event {i}/{len(event_urls)}: {event_url}\")\n",
    "        fights = get_fights_from_event(event_url)\n",
    "        all_fights.extend(fights)\n",
    "        time.sleep(2)  # Delay between event requests\n",
    "    return all_fights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For testing purposes, limit to the first 10 events. Remove or adjust 'limit_events' for full scraping.\n",
    "    fights_data = scrape_ufc_data(limit_events=1)\n",
    "    df = pd.DataFrame(fights_data)\n",
    "    df.to_csv('ufc_fight_history_extended.csv', index=False)\n",
    "    print(\"Saved extended UFC fight data to 'ufc_fight_history_extended.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
